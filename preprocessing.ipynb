{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.12'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import qdrant_client\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "\n",
    "from llama_index.embeddings import resolve_embed_model\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.node_parser import SimpleNodeParser, HierarchicalNodeParser\n",
    "import tiktoken\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use library\n",
    "# from llama_index import SimpleDirectoryReader\n",
    "# from llama_index import Document\n",
    "# from collections import defaultdict\n",
    "\n",
    "# loader = SimpleDirectoryReader(\n",
    "#     input_dir=\"/home/inbodyai/문서/Junhwi/Retrieval_Dataset/Papers\",\n",
    "#     required_exts=['.pdf'],\n",
    "#     recursive=True,\n",
    "#     filename_as_id=True,\n",
    "    \n",
    "# )\n",
    "# docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/4423 [00:03<06:21, 11.51it/s]invalid pdf header: b'\\t<!DO'\n",
      "incorrect startxref pointer(1)\n",
      "  3%|▎         | 124/4423 [00:36<09:10,  7.81it/s] invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(3)\n",
      "  3%|▎         | 152/4423 [00:41<10:35,  6.72it/s]Multiple definitions in dictionary at byte 0x1a8dd for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1aa8c for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1ac29 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1adcb for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1afaa for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1b16c for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1b34b for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1b515 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1b71f for key /MediaBox\n",
      "  4%|▎         | 156/4423 [00:42<17:17,  4.11it/s]invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(3)\n",
      "  7%|▋         | 302/4423 [01:27<10:22,  6.62it/s]  Overwriting cache for 0 55\n",
      " 11%|█▏        | 498/4423 [04:48<08:50,  7.39it/s]   Multiple definitions in dictionary at byte 0x19068 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x19211 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1939b for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x19515 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x196cf for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x19866 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x19a00 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x19c82 for key /MediaBox\n",
      " 17%|█▋        | 750/4423 [05:40<13:12,  4.64it/s]Overwriting cache for 0 1116\n",
      " 22%|██▏       | 961/4423 [06:20<14:53,  3.88it/s]Overwriting cache for 0 1116\n",
      " 34%|███▍      | 1493/4423 [08:53<10:39,  4.58it/s]Multiple definitions in dictionary at byte 0x16c78 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x16e48 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x16fb1 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1716a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x17340 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x174e1 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1772a for key /MediaBox\n",
      " 35%|███▍      | 1531/4423 [09:06<10:49,  4.45it/s]  Multiple definitions in dictionary at byte 0x24268 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x244cc for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x246dd for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x248c6 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24a5c for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24bea for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24da0 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24f46 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2519f for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x253f5 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x255ee for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2583f for key /MediaBox\n",
      " 39%|███▊      | 1705/4423 [09:46<22:50,  1.98it/s]Illegal character in Name Object (b'/GFEDCB+\\x14\\xd5')\n",
      "Illegal character in Name Object (b'/GFEDCB+\\x14\\xd5')\n",
      " 41%|████      | 1807/4423 [10:14<08:10,  5.33it/s]Multiple definitions in dictionary at byte 0x1a08e for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1a2f4 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1a4b6 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1a639 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1a79c for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1a99e for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1ab31 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1acac for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1ae86 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1b0a0 for key /MediaBox\n",
      " 44%|████▍     | 1945/4423 [10:53<22:23,  1.84it/s]invalid pdf header: b'\\r\\n%PD'\n",
      "incorrect startxref pointer(3)\n",
      " 53%|█████▎    | 2360/4423 [12:26<06:28,  5.31it/s]Multiple definitions in dictionary at byte 0x2a4a8 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2a6dd for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2a874 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2aa76 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2aca0 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2ae2a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2afc1 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2b19b for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2b37a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2b57c for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2b6fe for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2b970 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2bbca for key /MediaBox\n",
      " 53%|█████▎    | 2364/4423 [12:27<05:59,  5.73it/s]incorrect startxref pointer(1)\n",
      " 73%|███████▎  | 3226/4423 [15:37<06:02,  3.30it/s]Multiple definitions in dictionary at byte 0xfb733 for key /PageMode\n",
      " 77%|███████▋  | 3415/4423 [16:29<03:26,  4.89it/s]Multiple definitions in dictionary at byte 0x12e22c for key /PageMode\n",
      " 79%|███████▉  | 3514/4423 [16:59<03:35,  4.23it/s]Multiple definitions in dictionary at byte 0x24347 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x245be for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24748 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24932 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24aad for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24c97 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x24e5a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x25064 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x251fe for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x25470 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x25642 for key /MediaBox\n",
      " 85%|████████▍ | 3746/4423 [18:00<03:29,  3.23it/s]Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      "Advanced encoding /UniKS-UTF16-H not implemented yet\n",
      " 89%|████████▉ | 3949/4423 [18:46<01:52,  4.22it/s]invalid pdf header: b'<!DOC'\n",
      "incorrect startxref pointer(3)\n",
      " 91%|█████████ | 4023/4423 [19:06<01:05,  6.08it/s]Multiple definitions in dictionary at byte 0x209e2 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x20da4 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x210a6 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x212c7 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x21458 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x2163a for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x218dc for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x21bad for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x21e4e for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x22017 for key /MediaBox\n",
      " 93%|█████████▎| 4110/4423 [19:19<00:27, 11.27it/s]invalid pdf header: b'\\n\\n\\n\\n\\n'\n",
      "incorrect startxref pointer(3)\n",
      " 94%|█████████▍| 4171/4423 [19:29<00:28,  8.89it/s]Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      " 95%|█████████▍| 4181/4423 [19:31<00:34,  7.03it/s]Advanced encoding /B5pc-H not implemented yet\n",
      " 96%|█████████▌| 4242/4423 [19:39<00:18,  9.74it/s]Multiple definitions in dictionary at byte 0x128f2 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x12aa1 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x12c11 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x12e10 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x12fe2 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1320c for key /MediaBox\n",
      " 97%|█████████▋| 4285/4423 [19:46<00:13, 10.37it/s]invalid pdf header: b'\\n\\n\\n\\n\\n'\n",
      "incorrect startxref pointer(3)\n",
      " 98%|█████████▊| 4338/4423 [19:54<00:13,  6.38it/s]Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-H not implemented yet\n",
      "Advanced encoding /90ms-RKSJ-V not implemented yet\n",
      " 99%|█████████▉| 4376/4423 [20:01<00:06,  6.89it/s]invalid pdf header: b'\\r\\n\\r\\n<'\n",
      "incorrect startxref pointer(1)\n",
      " 99%|█████████▉| 4395/4423 [20:04<00:03,  7.82it/s]Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "Advanced encoding /KSCms-UHC-H not implemented yet\n",
      "100%|██████████| 4423/4423 [20:12<00:00,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom Pdf Loader(pyPDF)\n",
    "from llama_index.schema import Document\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = glob(\"/home/inbodyai/문서/Junhwi/Retrieval_Dataset/Papers/**/*.pdf\", recursive=True)\n",
    "docs = []\n",
    "\n",
    "for path in tqdm(input_dir):\n",
    "    \n",
    "    not_load_list = []\n",
    "    text = ''\n",
    "    try:\n",
    "        pdf = PdfReader(path)\n",
    "        file_name = os.path.basename(path)\n",
    "        \n",
    "        for page in range(pdf.pages.__len__()):\n",
    "            # 페이지를 텍스트에 추가\n",
    "            text += pdf.pages[page].extract_text()\n",
    "            \n",
    "        docs.append(Document(\n",
    "            text = text,\n",
    "            metadata = {'file_name' : file_name}\n",
    "        ))\n",
    "    except:\n",
    "        not_load_list.append(path)\n",
    "\n",
    "\n",
    "print(docs.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423 0\n"
     ]
    }
   ],
   "source": [
    "print(docs.__len__(), not_load_list.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4423\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "\n",
    "# 문서 하나로 병합\n",
    "prev_page = 0\n",
    "cnt = 0\n",
    "text = '\\n'\n",
    "documents_dict = defaultdict(str)\n",
    "original_dict = defaultdict(str)\n",
    "for doc in docs:\n",
    "    \n",
    "    text = doc.get_content()\n",
    "    original_dict[doc.metadata['file_name']] += text\n",
    "    # 유니코드 제거, 소문자화\n",
    "    text = unicodedata.normalize(\"NFKD\", text.lower())\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # \\n\\n이 없어질 때 까지 제거\n",
    "    while '..' in text:\n",
    "        text = text.replace('..', '.')\n",
    "    while '\\n\\n' in text:\n",
    "        text = text.replace('\\n\\n', '\\n')\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    while ' -\\n' in text:\n",
    "        text = text.replace(' -\\n', '')\n",
    "\n",
    "    while '-\\n' in text:\n",
    "        text = text.replace('-\\n', '')\n",
    "    documents_dict[doc.metadata['file_name']] += text\n",
    "    # documents_dict[doc.metadata['file_name']] += doc.get_content()\n",
    "    \n",
    "print(documents_dict.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "other_country_list = []\n",
    "documents = []\n",
    "except_list = []\n",
    "not_eng_list = []\n",
    "for file_name in documents_dict:\n",
    "    \n",
    "    text = documents_dict[file_name]\n",
    "    # text가 비어있으면 except_list에 추가하고 다음 문서로 넘어감(pdf reader 오류)\n",
    "    if text == \"\": \n",
    "        except_list.append(file_name)\n",
    "        continue\n",
    "    if file_name == '透析患者における従来の公式を用いて算出したTotal Body Water(TBW)と、Bioeletrical Impedance Analysis(BIA)による測定値の相違(InBody 3.0, 2004).pdf':\n",
    "        continue        \n",
    "    if '일본투석학회지' in file_name:\n",
    "        continue\n",
    "    if file_name == \"InBody_정밀도 논문 리스트(비교 대상에 따른 분류)_180803_Special Ver..pdf\":\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # 앞의 100글자로 언어 판단\n",
    "    try:\n",
    "        lang = detect(text[:100])\n",
    "    # 앞의 100글자로 판단이 어려우면 전체 문장으로 판단\n",
    "    except:\n",
    "        lang = detect(text)\n",
    "        \n",
    "    if lang in [\"ko\", \"ja\"]:\n",
    "        other_country_list.append(file_name)\n",
    "        \n",
    "    else:\n",
    "        # 100글자로 판단했지만, 완벽하지 않으므로 영어가 아닌것들은 리스트에 추가\n",
    "        if lang != \"en\":\n",
    "            # text가 아닌 file_name으로 한번 더 디텍트\n",
    "            if detect(file_name) in ['ko', 'ja']:\n",
    "                not_eng_list.append(file_name)\n",
    "                \n",
    "            else:\n",
    "                documents.append(Document(id_=f\"{file_name}\", text=text, metadata={\"file_name\": file_name}))\n",
    "                # try:\n",
    "                #     if detect(file_name[:7]) in ['ko', 'ja']:\n",
    "                #         not_eng_list.append(file_name)\n",
    "                #     else:\n",
    "                #         documents.append(Document(id_=f\"{file_name}\", text=text, metadata={\"file_name\": file_name}))\n",
    "                        \n",
    "                # except:\n",
    "                #     documents.append(Document(id_=f\"{file_name}\", text=text, metadata={\"file_name\": file_name}))\n",
    "                    \n",
    "        else:\n",
    "            documents.append(Document(id_=f\"{file_name}\", text=text, metadata={\"file_name\": file_name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4357, 43, 1, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.__len__(), except_list.__len__(), not_eng_list.__len__(), other_country_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2009_3.0&S20_The relationship between nocturnal polyuria and the distribution of body fluid- assessment by BIA.pdf',\n",
       " '2009_S20_The Relationship Between Nocturnal Polyuria and the Distribution of Body Fluid.pdf',\n",
       " 'Estimating survival in patients with cancer receiving palliative care- is analysis of body composition using bioimpedance helpful(InBody720, 2009).pdf',\n",
       " 'Multifrequency bioelectrical impedance fails to quantify sequestration of abdominal fluid(미기재, 1995).pdf',\n",
       " 'Evaluation of Segmental Bioelectrical Impedance Analysis(SBIA) for Measuring Muscle Distribution(InBody2.0, FFM, 1997).pdf',\n",
       " 'Assessment fat free mass using Segmental bioelectrical impedance analysis(미기재, FFM, 2000)_kor.pdf',\n",
       " 'Evaluation of MFBIA in Assessing Body Composition of Wrestlers(InBody520, FFM, 2010).pdf',\n",
       " '2020_720_Accuracy augmentation of body composition measurement by bioelectrical impedance analyzer in elderly population.pdf',\n",
       " 'Multifrequency bioelectrical impedance estimates the distribution of body water(미기재, TBW, 1995).pdf',\n",
       " 'Reference Data of Multi Frequencies Bioelectric Impedance Method in Japanese(InBody720, 2009).pdf',\n",
       " 'A New Method for Measuring Body Composition using Segmental Bioelectrical Impedance Analysis(InBody2.0, 1997).pdf',\n",
       " 'Impact of pretransplant nutritional status in patients undergoing liver transplantation(InBody720, 2010).pdf',\n",
       " 'Effect of a Combined Aerobic and Resistance Exercise Program on Cardiovascular Disease Risk Factors in Hypertensive Patients(InBody720, 2018).pdf',\n",
       " 'The Clinical Application of Bioelectrical Impedance Analysis in Patients with Critical Illness(InBodyS10, 2010).pdf',\n",
       " 'The prevalence of muscle wasting(sarcopenia) in PD patients varies with ethnicity due to differences in muscle mass measured by bioimpedance(InBody720, 2018).pdf',\n",
       " 'Comparisons of Predictive Equations for Resting Energy Expenditure in Patients with Cerebral Infarct during Acute Care(InBodyS10, 2015).pdf',\n",
       " 'Plasma visfatin levels are positively associated with circulating interleukin-6 in apparently healthy Korean women(InBody3.0, 2008).pdf',\n",
       " 'Abstract_Bedside methods vs DEXA for body composition measurement in COPD(미기재, 2002).pdf',\n",
       " 'Lean Body Mass and Body Fat Distribution in Participants With Chronic Low Back Pain(InBody2.0, 2000).pdf',\n",
       " '成長痛を有する患者における筋質量と体量支持指数の関係(InBody3.0, 2010).pdf',\n",
       " 'A Decline in Lower Extremity Lean Body Mass per Body Weight Is Characteristics of Women with Early Phase Osteoarthritis of the knee(InBody2.0, 2000).pdf',\n",
       " 'Combined Opto-Electronic Perometry and Bioimpedance to Measure Objectively the Effectiveness of a New Treatment Intervention for Chronic Secondary Leg Lymphedema(InBody3.0, 2002).pdf',\n",
       " 'The usefulness of walking for preventing sarcopenia in dieting postmenopausal women complaining of knee pain(InBody2.0, 2000).pdf',\n",
       " '1997_사장님_A new method for measuring body composition using Segmental BIA_InBody2.0_팔다리 길이.pdf',\n",
       " '1995_사장님_Multifrequency BIA estimates the distrubution of body water_중수법 실험.pdf',\n",
       " '1997_사장님_새로운 생체 전기 임피던스법.pdf',\n",
       " '1995_사장님_Multifrequency BIA fails to quantify sequestration of abdominal fluid_쥐 실험.pdf',\n",
       " '1997_사장님_Evaluation of Segmental Bioelectrical Impedance Analysis(SBIA) for Measuring Muscle Distribution.pdf',\n",
       " 'The calcineurin inhibitor tacrolimus activates the renal sodium chloride cotransporter to caise hypertension(InBody720, 2011).pdf',\n",
       " 'Geriatric nutritional risk index as a prognostic factor in peritoneal dialysis patients(InBody4.0, 2013).pdf',\n",
       " 'Measurement of fluid shift in CAPD patients using segmental bioelectrical impedance analysis(InBody2.0, 1999).pdf',\n",
       " '일본투석학회지_高精度体成分分析装置(InBodyS20)を用いた血液透析患者の体液量評価：生体電気インピーダンス(BIA)法はDWの指導になり得るか？(nBodyS20,2007)_JPN.pdf',\n",
       " 'The prevalence of muscle wasting (sarcopenia) in peritoneal dialysis patients varies with ethnicity due to differences in muscle mass measured by bioimpedance (720, 2017).pdf',\n",
       " 'Body Water Measurement with Segmental Bioelectrical Impedance Analysis on Assessment of Dry Weight in Hemodialysis Patients(InBody3.0, 2002).pdf',\n",
       " '膜素材の違いにより透析中のアミノ酸漏出に差があるだろうか(InBody3.0, 2005).pdf',\n",
       " 'Assessment of TBW from anthropometry-based equations using BI as reference Korean adult control and HD subjects(InBody2.0, 2001).pdf',\n",
       " '高精度体成分分析装置を用いた血液透析患者の体液量評価_BIA法はDWの指標になり得るか(InBodyS20, 2007).pdf',\n",
       " 'Serial changes in body composition in patients with chronic renal failure on peritoneal dialysis(InBody3.0, 2001).pdf',\n",
       " 'Comparison of bioimpedance analysis and dual energy X_ray absorptiometry body composition measurements in peritoneal dialysis patients according to edema(InBody4.0, 2013).pdf',\n",
       " '慢性腎不全の腹膜透析患者における体組成の継時的変化(InBody3.0, 2001).pdf',\n",
       " 'Assessment of body fluid component in hernodialyzed patients using body composition analyzer(InBody S20, 2007).pdf',\n",
       " 'Effects of premenstrual dysphoric disorder on the changes of energy intake and body composition(InBody3.0, 2005)_KOR.pdf',\n",
       " 'Fluid changes in pregnancy(미기재, 1995).pdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 보유한 문서들 중 pdf reader가 안 되는 것\n",
    "except_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Correlation between preoperative PG-SGA score and body composition in patients with gastric cancer.(720, 2017)(중어논문).pdf',\n",
       " '논문요약_국문화_09 영양_190122.pdf',\n",
       " '논문요약_국문화_14 운동생리_190207.pdf',\n",
       " '논문요약_국문화_12 고령자_190130.pdf',\n",
       " '논문요약_국문화_07 정형·재활_190827.pdf',\n",
       " '논문요약_국문화_15 기타_180829.pdf',\n",
       " '논문요약_국문화_08 암·완화_190122.pdf',\n",
       " '논문요약_국문화_02 신장·투석_181212.pdf',\n",
       " '논문요약_국문화_04 순환기_181226.pdf',\n",
       " '논문요약_국문화_13 소아_190207.pdf',\n",
       " '논문요약_국문화_01 정밀도_210118.pdf',\n",
       " '논문요약_국문화_05 호흡기_181224.pdf',\n",
       " '논문요약_국문화_10 산부인과_180821.pdf',\n",
       " '논문요약_국문화_03 소화기_181218.pdf',\n",
       " '논문요약_국문화_11 비뇨기과_180822.pdf',\n",
       " '논문요약_국문화_06 내분비_181227.pdf',\n",
       " 'Abstract_InBody_Body Composition in Pregnancy.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다른 나라 언어로 디텍트\n",
    "other_country_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy\n",
    "# from utils import remove_reference_part\n",
    "# # 1. REFERENCES|References\n",
    "# processing_doc_list_1, except_list_1, else_list_1 = remove_reference_part(\n",
    "#     pattern=r\"(.*)(REFERENCES|References)(.*)\",\n",
    "#     documents=documents,\n",
    "#     filter_str = '1'\n",
    "#     )\n",
    "# print(processing_doc_list_1.__len__(), except_list_1.__len__(), else_list_1.__len__())\n",
    "\n",
    "# # 2. REFERENCES|References\n",
    "# processing_doc_list_2, except_list_2, else_list_2 = remove_reference_part(\n",
    "#     pattern=r\"(.*)(REFERENCES|References)(.*)\",\n",
    "#     documents=except_list_1 + else_list_1,\n",
    "#     filter_str = ','\n",
    "#     )\n",
    "# print(processing_doc_list_2.__len__(), except_list_2.__len__(), else_list_2.__len__())\n",
    "\n",
    "# # 3. REFERENCES|References\n",
    "# processing_doc_list_3, except_list_3, else_list_3 = remove_reference_part(\n",
    "#     pattern=r\"(.*)(REFER ENCES|Referen ces|r  e  f  e  r  e  n  c  e  s|Refer ences|EFERENCES)(.*)\",\n",
    "#     documents=except_list_2 + else_list_2,\n",
    "#     filter_str = ','\n",
    "#     )\n",
    "# print(processing_doc_list_3.__len__(), except_list_3.__len__(), else_list_3.__len__())\n",
    "\n",
    "# final_doc_list = processing_doc_list_1 + processing_doc_list_2 + except_list_3 + else_list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40352.82946981868"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서의 평균 길이\n",
    "length = 0\n",
    "for i in range(documents.__len__()):\n",
    "    length += len(documents[i].text)\n",
    "    \n",
    "length / documents.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove references :  4242\n",
      "to need confirm :  115\n",
      "remove references :  71\n",
      "to need confirm :  44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def keyword_remove_regex(documents, pattern=None):\n",
    "\n",
    "    preprocessed_docs = []\n",
    "    else_list = []\n",
    "\n",
    "    # .* 를 사용하면 greedy하게 매칭되어서, references가 여러번 나오는 경우, 마지막 references부터 매칭됨\n",
    "    \n",
    "    if not pattern:\n",
    "        pattern = r\"^(.*)(r\\neferences|refer ences|REFERENCES|References)(.*)$\"\n",
    "\n",
    "    for i in range(documents.__len__()):\n",
    "        text = documents[i].text\n",
    "        matches = re.search(pattern, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        if matches:\n",
    "                                                                                                                                                                                                                                                                                       \n",
    "            preprocessed_docs.append(Document(\n",
    "                                    id_ = documents[i].id_,\n",
    "                                    text=matches.groups()[0],\n",
    "                                    metadata={'file_name' : documents[i].metadata['file_name']}\n",
    "            ))\n",
    "            \n",
    "        else:\n",
    "            else_list.append(documents[i])\n",
    "            \n",
    "    print(\"remove references : \", preprocessed_docs.__len__())\n",
    "    print(\"to need confirm : \", else_list.__len__())\n",
    "    \n",
    "    return preprocessed_docs, else_list\n",
    "\n",
    "preprocessed_docs_1, else_list_1 = keyword_remove_regex(documents, pattern = r\"^(.*)(r\\neferences|refer ences|REFERENCES|References)(.*)$\")\n",
    "preprocessed_docs_2, else_list_2 = keyword_remove_regex(else_list_1, pattern = r\"^(.*)(reference|reference s|reference|reference:)(.*)$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None Check\n",
    "\n",
    "final_doc_list = preprocessed_docs_1 + preprocessed_docs_2 + else_list_2\n",
    "\n",
    "for doc in final_doc_list:\n",
    "    if doc.text == '':\n",
    "        print(doc.id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약하는거 하나 만들자\n",
    "# 새로운 index로 저장?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence window parser\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "\n",
    "window_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,\n",
    "    window_metadata_key= \"window\" , \n",
    "    original_text_metadata_key= \"original_text\")\n",
    "\n",
    "nodes_window = window_parser.get_nodes_from_documents(final_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '' check\n",
    "for node in nodes_window:\n",
    "    if node.text == '':\n",
    "        print(node.id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_parser_512 = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=51) #10% overlap\n",
    "# node_parser_256 = SimpleNodeParser.from_defaults(chunk_size=256, chunk_overlap=25) #10% overlap\n",
    "# nodes_512 = node_parser_512.get_nodes_from_documents(final_doc_list)\n",
    "# nodes_256 = node_parser_256.get_nodes_from_documents(final_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical node parser\n",
    "from llama_index.node_parser import SimpleNodeParser, HierarchicalNodeParser, get_leaf_nodes\n",
    "\n",
    "# define hierarchical node\n",
    "node_parser_h = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[\n",
    "        512,\n",
    "        256,\n",
    "        128\n",
    "    ]\n",
    ")\n",
    "node_h = node_parser_h.get_nodes_from_documents(final_doc_list)\n",
    "leaf_nodes = get_leaf_nodes(node_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '' check\n",
    "for node in node_h:\n",
    "    if node.text == None:\n",
    "        print(node.id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leaf node에만 summarize_id를 추가함.\n",
    "summarizecontent를 새로운 index로 정의함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "for embed_name in tqdm([\"BAAI/bge-small-en-v1.5\"]): # \"BAAI/bge-small-en\" \"thenlper/gte-base\", \"jamesgpt1/sf_model_e5\", WhereIsAI/UAE-Large-V1 jamesgpt1/sf_model_e5\n",
    "    # for nodes, embed_size in zip([node_h, nodes_256], ['h', '256']):\n",
    "    embed_size = 'h_0314'\n",
    "    # nodes = nodes_window\n",
    "    nodes = node_h\n",
    "    embed_model = resolve_embed_model(f\"local:{embed_name}\")\n",
    "    \n",
    "    # token_counter = TokenCountingHandler(\n",
    "    #     tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\").encode,\n",
    "    #     verbose = True\n",
    "    # )\n",
    "    # callback_manager = CallbackManager([token_counter])\n",
    "    \n",
    "    service_context = ServiceContext.from_defaults(embed_model=embed_model)#, callback_manager=callback_manager)\n",
    "    set_global_service_context(service_context)\n",
    "    collection_name = os.path.basename(embed_name)\n",
    "\n",
    "    db = chromadb.PersistentClient(path=f'./Papers/chroma/{collection_name}_{embed_size}') # 저장경로 어떻게 할지?\n",
    "    chroma_collection =db.get_or_create_collection(f\"{collection_name}_{embed_size}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    # nodes = [nodes[i:i+10] for i in range(0, len(nodes), 10)]\n",
    "\n",
    "    # for nodes_ in tqdm(nodes):\n",
    "    docstore = SimpleDocumentStore()\n",
    "    docstore.add_documents(nodes)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(docstore=docstore,\n",
    "                                                vector_store=vector_store\n",
    "                                                )\n",
    "    if 'h' in embed_size:\n",
    "        index = VectorStoreIndex(nodes=leaf_nodes, storage_context=storage_context)\n",
    "    else:\n",
    "        index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)\n",
    "        \n",
    "    index.storage_context.persist(persist_dir=f'./Papers/chroma/{collection_name}_{embed_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=f'./Papers/chroma/{collection_name}_{embed_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
